# Reusable GitHub Actions workflow for building and deploying frontend to EC2
# Uses OIDC for authentication in all environments (dev/test/mainnet)
name: "Frontend Build and Deploy"

on:
  workflow_call:
    inputs:
      repo-name:
        description: 'Name of the repository/artifact'
        required: true
        type: string
      node-version:
        description: 'Node.js version to use'
        required: true
        type: string
      build-command:
        description: 'npm build script to run'
        required: true
        type: string
      s3-aws-region:
        description: 'AWS region for S3 operations'
        required: true
        type: string
      isMainnet:
        description: 'Flag to indicate mainnet deployment'
        required: true
        type: boolean
    secrets:
      EC2_SSH_KEY:
        description: 'Private SSH key to connect to EC2'
        required: true
      EC2_USER:
        description: 'Username for SSH on EC2'
        required: true
      EC2_HOST:
        description: 'EC2 hostname or IP'
        required: true
      CLOUDFLARE_API_TOKEN:
        description: 'Cloudflare API token for cache purge'
        required: true
      CLOUDFLARE_ZONE_ID:
        description: 'Cloudflare zone identifier'
        required: true

# Global environment variables
env:
  AWS_ROLE_ARN: "arn:aws:iam::471112752664:role/GitHubAction-AssumeRoleWithAction"
  AWS_REGION: "us-west-1"
  AWS_DEFAULT_DEPLOYMENT_PATH: "/data/treasurenet"
  MAINNET_DEPLOYMENT_PATH: "/home/ubuntu/treasurenet"
  BUCKET_NAME: "tn-deployment-file-archive"
  MAINNET_BUCKET_NAME: "tn-mainnet-deployment-file-archive"
  SERVICE_TAG: "services"
  NGINX_PATH: "/data/nginx"
  MAINNET_NGINX_PATH: "/home/ubuntu/nginx"
  DEV_SECRET_PREFIX: "dev/services"
  TESTNET_SECRET_PREFIX: "testnet/services"
  MAINNET_SECRET_PREFIX: "mainnet/services"

# Use OIDC for authentication and grant minimal permissions
permissions:
  id-token: write     # required for OIDC
  contents: read      # read repository code

jobs:
  build-and-deploy:
    runs-on: self-hosted
    steps:
      - name: "Initialize Environment Variables"
        run: |
          # Store current date and time for artifact versioning
          echo "DATE=$(date '+%Y%m%d')" >> $GITHUB_ENV
          echo "TIME=$(date '+%Y%m%d_%H%M%S')" >> $GITHUB_ENV

      - name: "Checkout Repository"
        uses: actions/checkout@v4

      - name: "Set Up Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ inputs.node-version }}

      - name: "Configure AWS Credentials (Dev/Test)"
        if: ${{ !inputs.isMainnet }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::381492270411:role/GitHubActionsOIDCRole
          aws-region: ${{ inputs.s3-aws-region }}

      - name: "Configure AWS Credentials (Mainnet)"
        if: ${{ inputs.isMainnet }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_ARN }}
          role-session-name: GitHub_to_AWS_via_FederatedOIDC
          aws-region: ${{ env.AWS_REGION }}
      - name: "Fetch Nginx Certificates"
        run: |
          # Determine secret prefix based on environment
          if [ "${{ inputs.isMainnet }}" = "true" ]; then
            prefix=${{ env.MAINNET_SECRET_PREFIX }}
          elif [ -z "$EC2_HOST" ]; then
            prefix=${{ env.DEV_SECRET_PREFIX }}
          else
            prefix=${{ env.TESTNET_SECRET_PREFIX }}
          fi
          # Retrieve and decode
          secret=$(aws secretsmanager get-secret-value \
            --secret-id "${prefix}/nginx" \
            --region ${{ inputs.s3-aws-region }} \
            --query SecretString --output text)
          echo "$secret" | jq -r .ssl_certificate > cloudflare.pem
          echo "$secret" | jq -r .ssl_key         > cloudflare.key

      - name: "Discover EC2 Instance"
        run: |
          # Only query EC2 if EC2_HOST not already set
          if [ -z "${EC2_HOST}" ]; then
            if [ "${{ inputs.isMainnet }}" = "true" ]; then
              # Query mainnet EC2 by specific tag
              public_dns=$(aws ec2 describe-instances \
                --region ${{ env.AWS_REGION }} \
                --filters "Name=tag:Name,Values=web-service-1-us-west-1" "Name=instance-state-name,Values=running" \
                --query "Reservations[].Instances[].PublicDnsName" \
                --output text)
              echo "EC2_HOST=$public_dns" >> $GITHUB_ENV
            else
              # Query dev/test EC2 by general tag
              public_ip=$(aws ec2 describe-instances \
                --region ${{ inputs.s3-aws-region }} \
                --filters "Name=tag:Name,Values=${{ env.SERVICE_TAG }}" "Name=instance-state-name,Values=running" \
                --query "Reservations[].Instances[].PublicIpAddress" \
                --output text)
              echo "EC2_HOST=$public_ip" >> $GITHUB_ENV
            fi
          fi
          # Ensure EC2_USER is set
          if [ -z "${EC2_USER}" ]; then
            echo "EC2_USER=ubuntu" >> $GITHUB_ENV
          fi

      - name: "Install Dependencies and Build"
        run: |
          npm install --silent
          npm run ${{ inputs.build-command }} --silent

      - name: "Package Build Artifacts"
        run: |
          tar -zcvf ./${{ inputs.repo-name }}.tar.gz -C dist .
          tar -zcvf ./${{ inputs.repo-name }}_nginx.tar.gz -C nginx .



      - name: "Configure SSH Key"
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa

      - name: "Upload Artifacts to S3"
        run: |
          # Choose bucket based on environment
          if [ "${{ inputs.isMainnet }}" = "true" ]; then
            bucket=${{ env.MAINNET_BUCKET_NAME }}
          else
            bucket=${{ env.BUCKET_NAME }}
          fi
          aws s3 cp cloudflare.pem  s3://$bucket/${{ inputs.repo-name }}/${{ env.DATE }}/cloudflare.pem
          aws s3 cp cloudflare.key  s3://$bucket/${{ inputs.repo-name }}/${{ env.DATE }}/cloudflare.key
          aws s3 cp ./${{ inputs.repo-name }}.tar.gz s3://$bucket/${{ inputs.repo-name }}/${{ env.DATE }}/${{ inputs.repo-name }}_${{ env.TIME }}.tar.gz
          aws s3 cp ./${{ inputs.repo-name }}_nginx.tar.gz s3://$bucket/${{ inputs.repo-name }}/${{ env.DATE }}/${{ inputs.repo-name }}_nginx_${{ env.TIME }}.tar.gz

      - name: "Deploy to EC2"
        run: |
          # Dev/Test: direct SSH
          if [ "${{ inputs.isMainnet }}" != "true" ]; then
            dest=${{ env.AWS_DEFAULT_DEPLOYMENT_PATH }}
            bkt=${{ env.BUCKET_NAME }}
            ssh -o StrictHostKeyChecking=no ${EC2_USER}@${EC2_HOST} << 'EOF'
              mkdir -p ${{ env.NGINX_PATH }}/certs
              aws s3 cp s3://${{ env.BUCKET_NAME }}/${{ inputs.repo-name }}/${{ env.DATE }}/cloudflare.pem ${{ env.NGINX_PATH }}/certs/cloudflare.pem
              aws s3 cp s3://${{ env.BUCKET_NAME }}/${{ inputs.repo-name }}/${{ env.DATE }}/cloudflare.key ${{ env.NGINX_PATH }}/certs/cloudflare.key
              aws s3 cp s3://$bkt/${{ inputs.repo-name }}/${{ env.DATE }}/${{ inputs.repo-name }}_${{ env.TIME }}.tar.gz /home/${EC2_USER}/${{ inputs.repo-name }}.tar.gz
              tar -zxf /home/${EC2_USER}/${{ inputs.repo-name }}.tar.gz -C $dest
              aws s3 cp s3://$bkt/${{ inputs.repo-name }}/${{ env.DATE }}/${{ inputs.repo-name }}_nginx_${{ env.TIME }}.tar.gz /home/${EC2_USER}/${{ inputs.repo-name }}_nginx.tar.gz
              tar -zxf /home/${EC2_USER}/${{ inputs.repo-name }}_nginx.tar.gz -C ${{ env.NGINX_PATH }}
          EOF
          else
            # Mainnet: via bastion
            bastion=jump-server-us-west-1
            ssh -o StrictHostKeyChecking=no -i ~/.ssh/tn-mainnet-gha ec2-user@$bastion << 'OUTER'
              ssh -o StrictHostKeyChecking=no -i ~/mainnet-nodes-us-west-1.pem ubuntu@${EC2_HOST} << 'INNER'
                mkdir -p ${{ env.MAINNET_NGINX_PATH }}/certs
                aws s3 cp s3://${{ env.MAINNET_BUCKET_NAME }}/${{ inputs.repo-name }}/${{ env.DATE }}/cloudflare.pem ${{ env.MAINNET_NGINX_PATH }}/certs/cloudflare.pem
                aws s3 cp s3://${{ env.MAINNET_BUCKET_NAME }}/${{ inputs.repo-name }}/${{ env.DATE }}/cloudflare.key ${{ env.MAINNET_NGINX_PATH }}/certs/cloudflare.key
                aws s3 cp s3://${{ env.MAINNET_BUCKET_NAME }}/${{ inputs.repo-name }}/${{ env.DATE }}/${{ inputs.repo-name }}_${{ env.TIME }}.tar.gz /home/ubuntu/${{ inputs.repo-name }}.tar.gz
                mkdir -p ${{ env.MAINNET_DEPLOYMENT_PATH }}/
                tar -zxf /home/ubuntu/${{ inputs.repo-name }}.tar.gz -C ${{ env.MAINNET_DEPLOYMENT_PATH }}
                aws s3 cp s3://${{ env.MAINNET_BUCKET_NAME }}/${{ inputs.repo-name }}/${{ env.DATE }}/${{ inputs.repo-name }}_nginx_${{ env.TIME }}.tar.gz /home/ubuntu/${{ inputs.repo-name }}_nginx.tar.gz
                mkdir -p ${{ env.MAINNET_NGINX_PATH }}/
                tar -zxf /home/ubuntu/${{ inputs.repo-name }}_nginx.tar.gz -C ${{ env.MAINNET_NGINX_PATH }}
          INNER
          OUTER
          fi

      - name: "Start Nginx Container"
        run: |
          if [ "${{ inputs.isMainnet }}" = "true" ]; then
            # Mainnet: use two-hop SSH to update compose and restart
            bastion=jump-server-us-west-1
            ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa ec2-user@$bastion << 'OUTER'
              ssh -o StrictHostKeyChecking=no -i ~/mainnet-nodes-us-west-1.pem ubuntu@${EC2_HOST} << 'INNER'
                cd ${{ env.MAINNET_NGINX_PATH }}
                # Patch volume paths in compose file
                sed -i \
                  -e 's#/data/nginx#/home/ubuntu/nginx#g' \
                  -e 's#/data/treasurenet#/home/ubuntu/treasurenet#g' \
                  docker-compose.yaml
                if docker compose ps | grep -q Up; then docker compose restart; else docker compose up -d; fi
              INNER
            OUTER
          else
            # Dev/Test: single SSH restart
            ssh -o StrictHostKeyChecking=no $EC2_USER@$EC2_HOST << 'EOF'
              cd ${{ env.NGINX_PATH }}
              if docker-compose ps | grep -q Up; then docker-compose restart; else docker-compose up -d; fi
            EOF
          fi

      - name: "Purge Cloudflare Cache"
        run: |
          curl -X POST "https://api.cloudflare.com/client/v4/zones/${CLOUDFLARE_ZONE_ID}/purge_cache" \
            -H "Authorization: Bearer ${CLOUDFLARE_API_TOKEN}" \
            -H "Content-Type: application/json" \
            --data '{"purge_everything":true}'
